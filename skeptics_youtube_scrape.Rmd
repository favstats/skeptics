---
title: "Scrape Comments"
output: html_notebook
---

# Packages

```{r}
#devtools::install_github("soodoku/tuber")

pacman::p_load(tidyverse, tuber, magrittr, stringi, qdapRegex, purrr, lubridate, ggthemes, httr, plyr, svMisc, janitor, tm, textstem, tidytext)

```



# Authentication

```{r}
key <- "AIzaSyDmV5jHx7c8GeAmpTkv6To_ERdkZ_HNl70"

client_id <- "580511698053-p8mu77kktkcvb503g9q737svu9gcq101.apps.googleusercontent.com"

client_key <- "Y0eW96yy-WRg90RZjVJS615o"

tuber::yt_oauth(client_id, client_key)

```

googleapis.com/youtube/analytics/v1/reports?ids=channel%3D%3DUC-yewGHQbNFpDrGM0diZOLA&start-date=2016-01-01&end-date=2017-06-13&metrics=subscribersLost%2CsubscribersGained&key=Y0eW96yy-WRg90RZjVJS615o

# Read TXT & extract URLs

```{r}

extract_video_ids <- function(video_list) {

video_list %<>% 
  stri_replace_all_fixed(" ", "") %>% 
  stri_replace_all_charclass("\\p{WHITE_SPACE}", "")

url <- rm_between(video_list, "URL:", "Description", extract = TRUE)

imp_vec <- as.vector(url[[1]]) %>% 
  str_replace(pattern = "(.*v\\=)", "")

return(imp_vec)
}

get_video_ids <- function(textfile) {
  readr::read_file(textfile) %>% 
  extract_video_ids()
}

```

# Extract URLs

```{r}
amazingatheist_ids  <- get_video_ids("text/amazingatheist.txt")
armored_ids         <- get_video_ids("text/armored.txt")
logicked_ids        <- get_video_ids("text/logicked.txt")
thunder_ids         <- get_video_ids("text/thunder.txt")
sargon_ids          <- get_video_ids("text/sargon.txt")
repzion_ids         <- get_video_ids("text/repzion.txt")
warski_ids          <- get_video_ids("text/warski.txt")
blairewhite_ids     <- get_video_ids("text/blairewhite.txt")
cultofdusty_ids     <- get_video_ids("text/cultofdusty.txt")
bps_ids             <- get_video_ids("text/bps.txt")
undoomed_ids        <- get_video_ids("text/undoomed.txt")
aronra_ids          <- get_video_ids("text/aronra.txt")
dillahunty_ids      <- get_video_ids("text/dillahunty.txt")
thinkingatheist_ids <- get_video_ids("text/thinkingatheist.txt")
molymeme_ids <- get_video_ids("text/molymeme.txt")



```

# GetAllComments

```{r}
get_all_comments2 <- function(video_id = NULL, ...) {
  yt_check_token <- function() {
    app_token <- getOption("google_token")
    if (is.null(app_token)) stop("Please get a token using yt_oauth().\n")
  }

  tuber_GET <- function(path, query, ...) {
    yt_check_token()

    req <- GET(
      "https://www.googleapis.com", path = paste0("youtube/v3/", path),
      query = query, config(token = getOption("google_token")), ...
    )

    tuber_check <- function(req) {
      if (req$status_code < 400) return(invisible())

      stop("HTTP failure: ", req$status_code, "\n", call. = FALSE)
    }

    tuber_check(req)
    res <- content(req)

    res
  }

  querylist <- list(
    videoId = video_id, part = "id,replies,snippet",
    maxResults = 100
  )
  res <- tuber_GET("commentThreads", querylist, ...)
  simple_res <- lapply(res$items, function(x) {
    unlist(x$snippet$topLevelComment$snippet)
  })
  simpler_res <- ldply(simple_res, rbind)
  simpler_res$parentId <- NA
  n_replies <- sapply(res$items, function(x) {
    unlist(x$snippet$totalReplyCount)
  })
  if (sum(n_replies) > 1) {
    replies <- lapply(res$items[n_replies > 0], function(x) {
      unlist(x$replies$comments)
    })
    simpler_rep <- ldply(replies, rbind)
    names(simpler_rep) <- gsub("snippet.", "", names(simpler_rep))
    simpler_rep <- subset(simpler_rep, select = -c(
      kind,
      etag, id
    ))
    agg_res <- plyr::rbind.fill(simpler_res, simpler_rep)
  }
  agg_res <- simpler_res
  page_token <- res$nextPageToken
  while (is.character(page_token)) {
    querylist$pageToken <- page_token
    a_res <- tuber_GET("commentThreads", querylist, ...)
    simple_res <- lapply(a_res$items, function(x) {
      unlist(x$snippet$topLevelComment$snippet)
    })
    simpler_res <- ldply(simple_res, rbind)
    simpler_res$parentId <- NA
    n_replies <- sapply(a_res$items, function(x) {
      unlist(x$snippet$totalReplyCount)
    })
    if (sum(n_replies) > 1) {
      replies <- lapply(a_res$items[n_replies > 0], function(x) {
        unlist(x$replies$comments)
      })
      simpler_rep <- ldply(replies, rbind)
      names(simpler_rep) <- gsub("snippet.", "", names(simpler_rep))
      simpler_rep <- subset(simpler_rep, select = -c(
        kind,
        etag, id
      ))
      agg_res <- plyr::rbind.fill(simpler_res, simpler_rep, agg_res)
      page_token <- a_res$nextPageToken
    }
    agg_res <- plyr::rbind.fill(simpler_res, agg_res)
    page_token <- a_res$nextPageToken
  }
  agg_res %>%
    as.data.frame()
}

```


# Scraper Function

```{r}

# ids <- c(amazingatheist_ids[1:2], "ss", amazingatheist_ids[3])
readkey <- function() {
    line <- readline(prompt = "Are you there? Press [enter]")
}



authentication <- function() {
  beepr::beep(5)
  readkey()
  auth_token <- ".httr-oauth"
  if (file.exists(auth_token)) file.remove(auth_token)

  client_id <- "580511698053-p8mu77kktkcvb503g9q737svu9gcq101.apps.googleusercontent.com"

  client_key <- "Y0eW96yy-WRg90RZjVJS615o"

  tuber::yt_oauth(client_id, client_key)
}


extract_comments <- function(ids, n, youtuber) {
  comments <- list()
  cat("Starting loop. Sit back and get a tea. This might take a while...  ( ͡° ͜ʖ ͡°)\n")
  for (jj in n:length(ids)) {
#    svMisc::progress(jj)
    outcome <- tryCatch({
      get_all_comments2(ids[jj])
    }, 
    error = function(e){
      e
    })
    if (is.data.frame(outcome)) { #checking if an error occured
      comments[[jj]] <- outcome
    } else if (str_detect(outcome$message, "401")) { #if Error Message include 401
      cat("\n Loop stopped on iteration ", jj)
      cat("\n", outcome$message)
      message("\n YouTube Authentication needed!")
      cat("\n Saving data for safety...")
      save(comments, file = paste0("data/", youtuber, "_comments_",  n, "_", jj, ".Rdata"))   
      cat("\t Done! \n")
      authentication()
      cat("\n Waiting for 10 seconds...")
      Sys.sleep(10)                
      cat("\t Done! \n Continuing...")
      comments[[jj]] <- get_all_comments2(ids[jj])
    } else if (str_detect(outcome$message, "400")) { #if Error Message include 400
        beepr::beep(9)
      cat("\n", outcome$message)
      cat("\n Encountered problem at iteration ", jj)
      cat("\n Saving data for safety...")
      save(comments, file = paste0("data/", youtuber, "_comments_",  n, "_", jj, ".Rdata"))   
      cat("\t Done!")
      cat("\n Waiting for 20 seconds and..")
      Sys.sleep(20)
      cat("\t trying again\n")
      comments[[jj]] <- get_all_comments2(ids[jj])
      cat("\n Puuh... this worked. Deleting file.\n")
      file.remove(paste0("data/", youtuber, "_comments_",  n, "_", jj, ".Rdata"))
    } else if (str_detect(outcome$message, "403")) { #if Error Message include 403
        beepr::beep(11)      
      cat("\n", outcome$message)
      cat("\n Encountered problem at iteration ", jj)
      Sys.sleep(10)
      cat("\n Skipping", jj, "\n")
      next
    } # TODO(favstats): failsafe what happens if another error happens?
    cat(paste0("\t", jj)) 
  }   # End of the for loop
  cat("\n Binding rows. Might take while.. \t")
  comments <- suppressWarnings(bind_rows(comments))
  cat("Done!\n ")
  cat("\n Saving data one last time...")
  save(comments, file = paste0("data/", youtuber, "_comments_",  n, "_", jj, ".Rdata"))
  cat("\t Done!\n")
  if (jj == length(ids)) beepr::beep(8)
  cat(paste("\n Congratulations! You just succesfully mined", nrow(comments), "comments from", jj, "YouTube Videos"))
  return(comments)
}



```



# Armored Skeptic

## Scraping

```{r}
armored_comments <- list()
for (jj in seq_along(armored_ids)) {  
  logicked_comments[[jj]] <- get_all_comments2(logicked_ids[jj])
  cat(jj)
}
```

# Amazing Atheist

## Scraping

```{r}

## Try 1
amazingatheist_comments1 <- extract_comments(amazingatheist_ids, 1, "amazingatheist")

## Try 2

amazingatheist_comments2 <- extract_comments(amazingatheist_ids, 934, "amazingatheist")

amazingatheist_comments <- rbind(
  suppressWarnings(bind_rows(get(load("data/amazingatheist_comments_1_238.Rdata")))),
  suppressWarnings(bind_rows(get(load("data/amazingatheist_comments_1_470.Rdata")))),
  suppressWarnings(bind_rows(get(load("data/amazingatheist_comments_1_748.Rdata")))),
  suppressWarnings(bind_rows(get(load("data/amazingatheist_comments_1_848.Rdata")))),
  suppressWarnings(bind_rows(get(load("data/amazingatheist_comments_1_934.Rdata")))),
  suppressWarnings(bind_rows(get(load("data/amazingatheist_comments_934_1448.Rdata"))))
)

save(amazingatheist_comments, file = "data/amazingatheist_comments.Rdata")
```

# Logicked

## Scraping

```{r}

logicked_comments1 <- extract_comments(logicked_ids, 1, "thunder")


```



# thunderfoot

## Scraping

```{r}

thunder_comments1 <- extract_comments(thunder_ids, 1, "thunder")

```

# Sargon of Akkad

## Scraping

```{r}

## Try 1
sargon_comments1 <- extract_comments(sargon_ids, 1, "sargon")

sargon_comments <- sargon_comments1

```

# Mr Repzion

## Scraping

```{r}

## Try 1
repzion_comments1 <- extract_comments(repzion_ids, 1, "repzion")

repzion_comments <- repzion_comments1


```

# Warski

## Scraping

```{r}
## Try 1
warski_comments1 <- extract_comments(warski_ids, 183, "warski")

warski_comments <- rwarski_comments1


```

# Blaire White

## Scraping

```{r}

blairewhite_comments1 <- extract_comments(blairewhite_ids, 1, "blairewhite")

blairewhite_comments <- blairewhite_comments1

```

# Cult of Dusty

## Scraping

```{r}

cultofdusty_comments1 <- extract_comments(cultofdusty_ids, 1, "cultofdusty")

cultofdusty_comments1 <- extract_comments(cultofdusty_ids, 122, "cultofdusty")

cultofdusty_comments <- plyr::rbind.fill(
  get(load("data/cultofdusty_comments_1_123.Rdata")),
  get(load("data/cultofdusty_comments_122_319.Rdata"))  
)

cultofdusty_comments %<>% 
  distinct()

save(cultofdusty_comments, file = "data/cultofdusty_comments.Rdata")

```

# Black Pigeon Speaks

## Scraping

```{r}

bps_comments1 <- extract_comments(bps_ids, 1, "bps")

```

# Undoomedd

## Scraping

```{r}

undoomed_comments1 <- extract_comments(undoomed_ids, 1, "undoomed")

```

# Aron Ra

## Scraping

```{r}

aronra_comments1 <- extract_comments(aronra_ids, 1, "aronra")

```

# Matt Dillahunty

## Scraping

```{r}
dillahunty_comments1 <- extract_comments(dillahunty_ids, 1, "dillahunty")

```

# The Thinking Atheist

## Scraping

```{r}
thinkingatheist_comments1 <- extract_comments(thinkingatheist_ids, 1, "thinkingatheist")

```



# Stefan Molyneux

## Scraping

```{r}
molymeme_comments1 <- extract_comments(molymeme_ids, 1, "molymeme")

```

# old scraper

- ACHTUNG! REMOVE THE (POSSIBLE) DUPLICATES!
- Test if 45 never works
- Test if 9 never works (in the second sample of 221)


```{r}

# extract function
extract_comments <- function(ids) {
  comments <- list()
  for (jj in seq_along(ids)) {
    comments[[jj]] <- get_all_comments2(ids[jj])
    cat(paste0("\t", jj))
  }
  comments <- suppressWarnings(bind_rows(comments))
  message(paste("\n\nIteration stopped at:", jj))
  return(comments)
}

ww <- c(amazingatheist_ids[1:2], "ss", amazingatheist_ids[1:3])

try_catch(extract_comments(ww), 
          .e = ~ paste0("There is an error: ", .x), 
          .w = ~ paste0("This is a warning: ", .x))



# Error : HTTP failure: 401 <- AUTH ERROR

# Error : HTTP failure: 400 <- nonproblematic
# Error : HTTP failure: 403 <- nonproblematic

# Try 1
# missed
c(44, 60, 61, 62, 73, 74, 88) # 7
amazingatheist_comments1 <- extract_comments(amazingatheist_ids)

count(unique(amazingatheist_comments1$videoId))

save(amazingatheist_comments1, file = "data/amazingatheist_comments1.Rdata")

# Try 2
 
amazingatheist_ids1 <- amazingatheist_ids[145:length(amazingatheist_ids)]
amazingatheist_comments2 <-  extract_comments(amazingatheist_ids1)

save(amazingatheist_comments2, file = "data/amazingatheist_comments2.Rdata")

# Try 3

amazingatheist_ids3 <- amazingatheist_ids1[jj:length(amazingatheist_ids1)]
amazingatheist_comments3 <-  extract_comments(amazingatheist_ids3)

save(amazingatheist_comments3, file = "data/amazingatheist_comments3.Rdata")




# old
 
amazingatheist_comments1 <- list()
for (jj in seq_along(amazingatheist_ids)) {  
  amazingatheist_comments1[[jj]] <- get_all_comments2(amazingatheist_ids[jj])
  cat(paste0("\t",jj))
}


save(amazingatheist_comments2, file = "data/amazingatheist_comments2.Rdata")

extract_comments <- function(ids) {
  comments <- list()
for (jj in seq_along(ids)) {  
  comments[[jj]] <- get_all_comments2(ids[jj])
  cat(paste0("\t",jj))
  }
  comments <- suppressWarnings( bind_rows(comments) )
  message(paste("\n\nIteration stopped at:", jj))
  return(comment)
}

ww <- c(amazingatheist_ids[1:2], "ss", amazingatheist_ids[1:3])
extract_comments(ww)

try_catch(lol <- extract_comments(ww), 
          .e = function(e){
            print(paste0("There is an error: ", e))
            print("Ok, let's save this")
            save(lol, "lol.Rdata") # commented to prevent from log.txt creation on your machine
            print(paste("log saved on log.txt at", time))
            print("let's move on now")
          })

x <- "HTTP sdg"
s <- str_detect(x, "HTTP")
# Stop if .x is numeric
b  <- 20
# Warn if .x is not equal to 10
warn_if_not(.x = x, 
        .p = ~ str_detect(.x, "fllap") , 
        msg = "b should be 10")

warn_if(.x = x, 
        .p = ~ str_detect(.x, "HTTP"), 
        msg = "b should be 10")

try_catch(fluuur <- extract_comments(ww), 
          .e = ~ warn_if(.x = .x, 
               .p = ~ if (str_detect(.x, "HTTP failure: 404")) save(fluuur, "fluuur.Rdat"), 
               msg = "I did it!"))

ids <- c("1", "3", "7", "9")

errors <- c("Error: Try Again", "Error: Stop for Loop")

  numbers <- list()
for (jj in seq_along(ids)) {  
  numbers[[jj]] <- as.numeric(ids[jj])
  if (jj == sample(1:4, 1)) stop(sample(errors, 1))
  }

numbers

  numbers <- list()
for (jj in seq_along(ids)) {  
  numbers[[jj]] <- as.numeric(ids[jj])
  }
  
  numbers
  
  safely <- function(fn, ..., max_attempts = 5) {
  function(...) {
    this_env <- environment()
    for(i in seq_len(max_attempts)) {
      ok <- tryCatch({
          assign("result", fn(...), envir = this_env)
          TRUE
        },
        error = function(e) {
          FALSE
        }
      )
      if(ok) {
        return(this_env$result)
      }
    }
    msg <- sprintf(
      "%s failed after %d tries; returning NULL.",
      deparse(match.call()),
      max_attempts
    )
    warning(msg)
    NULL
  }
  }
  
  
```
